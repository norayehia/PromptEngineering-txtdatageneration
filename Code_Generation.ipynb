{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoW7WL7DZNan7JZMNjs+if",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norayehia/PromptEngineering-txtdatageneration/blob/main/Code_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMil8WLIn-0H",
        "outputId": "7140ce96-bae6-4bd5-db7c-ee4c4048a860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Specify the model for code generation\n",
        "#model_name = \"bigcode/starcoder\"  # You can also use \"Salesforce/codegen-2B-multi\"\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"Salesforce/codegen-2B-multi\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the model and tokenizer\n",
        "print(\"Loading model and tokenizer...\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Prompt for code generation\n",
        "prompt = \"\"\"# Python function to calculate factorial using recursion\n",
        "def factorial(n):\"\"\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate code based on the prompt\n",
        "print(\"Generating Python code...\")\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=100,          # Adjust the maximum token length\n",
        "    temperature=0.7,         # Controls randomness\n",
        "    num_return_sequences=1,  # Number of completions\n",
        "    do_sample=True,          # Enable sampling\n",
        ")\n",
        "\n",
        "# Decode and display the generated Python code\n",
        "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Code:\\n\")\n",
        "print(generated_code)"
      ],
      "metadata": {
        "id": "r3dhhfAHoPtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating code snippets based on user prompts.Providing auto-\n",
        "\n",
        "completions for functions.Suggesting improvements to existing code."
      ],
      "metadata": {
        "id": "Qhm94r9Vr-sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Specify the model for code generation\n",
        "model_name = \"Salesforce/codegen-2B-multi\"\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def generate_code(prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generate Python code based on a given prompt.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def auto_complete_function(prompt):\n",
        "    \"\"\"\n",
        "    Provide auto-completion for an unfinished function.\n",
        "    \"\"\"\n",
        "    print(\"\\nGenerating auto-completion for your function...\")\n",
        "    return generate_code(prompt, max_length=150, temperature=0.5)\n",
        "\n",
        "\n",
        "def suggest_improvements(code_snippet):\n",
        "    \"\"\"\n",
        "    Suggest improvements for an existing code snippet.\n",
        "    \"\"\"\n",
        "    print(\"\\nSuggesting improvements for your code snippet...\")\n",
        "    prompt = f\"\"\"# Here is the original code:\n",
        "{code_snippet}\n",
        "\n",
        "# Here is the improved version with better practices:\n",
        "\"\"\"\n",
        "    return generate_code(prompt, max_length=150, temperature=0.7)\n",
        "\n",
        "\n",
        "def interactive_mode():\n",
        "    \"\"\"\n",
        "    Interactive mode to select tasks and input prompts.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        print(\"\\nChoose a task:\")\n",
        "        print(\"1. Generate a code snippet\")\n",
        "        print(\"2. Auto-complete a function\")\n",
        "        print(\"3. Suggest improvements for code\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1/2/3/4): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            user_prompt = input(\"\\nEnter a code prompt to generate a snippet:\\n\")\n",
        "            result = generate_code(user_prompt)\n",
        "            print(\"\\nGenerated Code Snippet:\\n\")\n",
        "            print(result)\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            user_prompt = input(\"\\nEnter the beginning of a function to auto-complete:\\n\")\n",
        "            result = auto_complete_function(user_prompt)\n",
        "            print(\"\\nAuto-Completed Function:\\n\")\n",
        "            print(result)\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            user_code = input(\"\\nEnter your code snippet for improvement suggestions:\\n\")\n",
        "            result = suggest_improvements(user_code)\n",
        "            print(\"\\nSuggested Improvements:\\n\")\n",
        "            print(result)\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            print(\"\\nExiting. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"\\nInvalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "# Run the interactive mode\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_mode()"
      ],
      "metadata": {
        "id": "h6CSjQZ7sTu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e55c67-c06f-46d1-c58c-7d523a1f6338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at Salesforce/codegen-2B-multi were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.20.attn.causal_mask', 'transformer.h.21.attn.causal_mask', 'transformer.h.22.attn.causal_mask', 'transformer.h.23.attn.causal_mask', 'transformer.h.24.attn.causal_mask', 'transformer.h.25.attn.causal_mask', 'transformer.h.26.attn.causal_mask', 'transformer.h.27.attn.causal_mask', 'transformer.h.28.attn.causal_mask', 'transformer.h.29.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.30.attn.causal_mask', 'transformer.h.31.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Choose a task:\n",
            "1. Generate a code snippet\n",
            "2. Auto-complete a function\n",
            "3. Suggest improvements for code\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1/2/3/4): 1\n",
            "\n",
            "Enter a code prompt to generate a snippet:\n",
            "Write code of divide two number then make list fill it with first ouput\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Code Snippet:\n",
            "\n",
            "Write code of divide two number then make list fill it with first ouput and second output.\n",
            "#\n",
            "# Example 1\n",
            "# Input: [2, 3, 5, 6, 8], 2\n",
            "# Output: [2, 2, 3, 1, 2, 2, 5, 6, 1, 8]\n",
            "#\n",
            "# Example 2\n",
            "# Input: [2, 3, 5, 6, 8], 3\n",
            "# Output: [2, 2, 2,\n",
            "\n",
            "Choose a task:\n",
            "1. Generate a code snippet\n",
            "2. Auto-complete a function\n",
            "3. Suggest improvements for code\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1/2/3/4): 1\n",
            "\n",
            "Enter a code prompt to generate a snippet:\n",
            "Write code of divide two number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Code Snippet:\n",
            "\n",
            "Write code of divide two number to int\n",
            "func Div(a int, b int) int {\n",
            "\tif b == 0 {\n",
            "\t\tfmt.Println(\"Divide by zero\")\n",
            "\t\treturn 0\n",
            "\t}\n",
            "\tr := a / b\n",
            "\tif a%b == 0 {\n",
            "\t\treturn r\n",
            "\t}\n",
            "\treturn r + 1\n",
            "}\n",
            "\n",
            "// code of print div result\n",
            "func Print(a int, b int) {\n",
            "\tf\n",
            "\n",
            "Choose a task:\n",
            "1. Generate a code snippet\n",
            "2. Auto-complete a function\n",
            "3. Suggest improvements for code\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1/2/3/4): 4\n",
            "\n",
            "Exiting. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}